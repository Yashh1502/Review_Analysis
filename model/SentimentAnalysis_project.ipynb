{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auggvjlPbRxN"
   },
   "source": [
    "## Plan of Action\n",
    "\n",
    "\n",
    "1.   We are using **Amazon Alexa Reviews dataset (3150 reviews)**, that contains: **customer reviews, rating out of 5**, date of review, Alexa variant \n",
    "2.   First we  **generate sentiment labels: positive/negative**, by marking *positive for reviews with rating >3 and negative for remaining*\n",
    "3. Then, we **clean dataset through Vectorization Feature Engineering** (TF-IDF) - a popular technique\n",
    "4. Post that, we use **Support Vector Classifier for Model Fitting** and check for model performance (*we are getting >90% accuracy*)\n",
    "5. Last, we use our model to do **predictions on real Amazon reviews** using: a simple way and then a fancy way\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-z5wIxkEUaKx"
   },
   "source": [
    "## Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BQgQHj_g_Hrc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1653983649953,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "zwnN_CQGzRJM",
    "outputId": "a0adb4e9-3d0f-4634-c5b8-8c1ea7df227b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Review</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Candes 12 L Room/Personal Air Cooler??????(Whi...</td>\n",
       "      <td>3999</td>\n",
       "      <td>5</td>\n",
       "      <td>super!</td>\n",
       "      <td>great cooler excellent air flow and for this p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Candes 12 L Room/Personal Air Cooler??????(Whi...</td>\n",
       "      <td>3999</td>\n",
       "      <td>5</td>\n",
       "      <td>awesome</td>\n",
       "      <td>best budget 2 fit cooler nice cooling</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candes 12 L Room/Personal Air Cooler??????(Whi...</td>\n",
       "      <td>3999</td>\n",
       "      <td>3</td>\n",
       "      <td>fair</td>\n",
       "      <td>the quality is good but the power of air is de...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Candes 12 L Room/Personal Air Cooler??????(Whi...</td>\n",
       "      <td>3999</td>\n",
       "      <td>1</td>\n",
       "      <td>useless product</td>\n",
       "      <td>very bad product its a only a fan</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candes 12 L Room/Personal Air Cooler??????(Whi...</td>\n",
       "      <td>3999</td>\n",
       "      <td>3</td>\n",
       "      <td>fair</td>\n",
       "      <td>ok ok product</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205044</th>\n",
       "      <td>cello Pack of 18 Opalware Cello Dazzle Lush Fi...</td>\n",
       "      <td>1299</td>\n",
       "      <td>5</td>\n",
       "      <td>must buy!</td>\n",
       "      <td>good product</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205045</th>\n",
       "      <td>cello Pack of 18 Opalware Cello Dazzle Lush Fi...</td>\n",
       "      <td>1299</td>\n",
       "      <td>5</td>\n",
       "      <td>super!</td>\n",
       "      <td>nice</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205046</th>\n",
       "      <td>cello Pack of 18 Opalware Cello Dazzle Lush Fi...</td>\n",
       "      <td>1299</td>\n",
       "      <td>3</td>\n",
       "      <td>nice</td>\n",
       "      <td>very nice and fast delivery</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205047</th>\n",
       "      <td>cello Pack of 18 Opalware Cello Dazzle Lush Fi...</td>\n",
       "      <td>1299</td>\n",
       "      <td>5</td>\n",
       "      <td>just wow!</td>\n",
       "      <td>awesome product</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205048</th>\n",
       "      <td>cello Pack of 18 Opalware Cello Dazzle Lush Fi...</td>\n",
       "      <td>1299</td>\n",
       "      <td>4</td>\n",
       "      <td>value-for-money</td>\n",
       "      <td>very good but mixing bowl not included is one ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205049 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             product_name  product_price  \\\n",
       "0       Candes 12 L Room/Personal Air Cooler??????(Whi...           3999   \n",
       "1       Candes 12 L Room/Personal Air Cooler??????(Whi...           3999   \n",
       "2       Candes 12 L Room/Personal Air Cooler??????(Whi...           3999   \n",
       "3       Candes 12 L Room/Personal Air Cooler??????(Whi...           3999   \n",
       "4       Candes 12 L Room/Personal Air Cooler??????(Whi...           3999   \n",
       "...                                                   ...            ...   \n",
       "205044  cello Pack of 18 Opalware Cello Dazzle Lush Fi...           1299   \n",
       "205045  cello Pack of 18 Opalware Cello Dazzle Lush Fi...           1299   \n",
       "205046  cello Pack of 18 Opalware Cello Dazzle Lush Fi...           1299   \n",
       "205047  cello Pack of 18 Opalware Cello Dazzle Lush Fi...           1299   \n",
       "205048  cello Pack of 18 Opalware Cello Dazzle Lush Fi...           1299   \n",
       "\n",
       "        Rate           Review  \\\n",
       "0          5           super!   \n",
       "1          5          awesome   \n",
       "2          3             fair   \n",
       "3          1  useless product   \n",
       "4          3             fair   \n",
       "...      ...              ...   \n",
       "205044     5        must buy!   \n",
       "205045     5           super!   \n",
       "205046     3             nice   \n",
       "205047     5        just wow!   \n",
       "205048     4  value-for-money   \n",
       "\n",
       "                                                  Summary Sentiment  \n",
       "0       great cooler excellent air flow and for this p...  positive  \n",
       "1                   best budget 2 fit cooler nice cooling  positive  \n",
       "2       the quality is good but the power of air is de...  positive  \n",
       "3                       very bad product its a only a fan  negative  \n",
       "4                                           ok ok product   neutral  \n",
       "...                                                   ...       ...  \n",
       "205044                                       good product  positive  \n",
       "205045                                               nice  positive  \n",
       "205046                        very nice and fast delivery  positive  \n",
       "205047                                    awesome product  positive  \n",
       "205048  very good but mixing bowl not included is one ...   neutral  \n",
       "\n",
       "[205049 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset-SA.csv') \n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS_QZCeEX45f"
   },
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1653983686056,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "VlIYFkfUzbMx",
    "outputId": "a5a5bab6-1789-43c6-82f8-ac3c50bbea19"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>super!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fair</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>useless product</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fair</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205044</th>\n",
       "      <td>must buy!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205045</th>\n",
       "      <td>super!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205046</th>\n",
       "      <td>nice</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205047</th>\n",
       "      <td>just wow!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205048</th>\n",
       "      <td>value-for-money</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205049 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Review  Sentiment\n",
       "0                super!          5\n",
       "1               awesome          5\n",
       "2                  fair          3\n",
       "3       useless product          1\n",
       "4                  fair          3\n",
       "...                 ...        ...\n",
       "205044        must buy!          5\n",
       "205045           super!          5\n",
       "205046             nice          3\n",
       "205047        just wow!          5\n",
       "205048  value-for-money          4\n",
       "\n",
       "[205049 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df[['Review','Rate']]\n",
    "dataset.columns = ['Review', 'Sentiment']\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NOZZDqWe0zKZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rpate\\AppData\\Local\\Temp\\ipykernel_36840\\1084019159.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['Sentiment'] = compute_sentiments(dataset['Sentiment'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>super!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>useless product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205044</th>\n",
       "      <td>must buy!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205045</th>\n",
       "      <td>super!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205046</th>\n",
       "      <td>nice</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205047</th>\n",
       "      <td>just wow!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205048</th>\n",
       "      <td>value-for-money</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205049 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Review  Sentiment\n",
       "0                super!          1\n",
       "1               awesome          1\n",
       "2                  fair          0\n",
       "3       useless product          0\n",
       "4                  fair          0\n",
       "...                 ...        ...\n",
       "205044        must buy!          1\n",
       "205045           super!          1\n",
       "205046             nice          0\n",
       "205047        just wow!          1\n",
       "205048  value-for-money          1\n",
       "\n",
       "[205049 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to compute sentiments based on numeric labels\n",
    "def compute_sentiments(labels):\n",
    "    sentiments = []\n",
    "    for label in labels:\n",
    "        if label > 3.0:\n",
    "            sentiment = 1\n",
    "        elif label <= 3.0:\n",
    "            sentiment = 0\n",
    "        sentiments.append(sentiment)\n",
    "    return sentiments\n",
    "\n",
    "# Apply the compute_sentiments function to the 'Sentiment' column\n",
    "dataset['Sentiment'] = compute_sentiments(dataset['Sentiment'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 602,
     "status": "ok",
     "timestamp": 1653983739642,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "pIjR4QPo04Aw",
    "outputId": "7ba76658-40bb-45ba-d326-21be720f87c3"
   },
   "outputs": [],
   "source": [
    "#dataset['Sentiment'] = compute_sentiments(dataset.Sentiment)\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1653983745856,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "KoyOZ3Ha1AGz",
    "outputId": "4b7dfce1-71b5-48a3-e22f-bb5a92cc98f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>super!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>useless product</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fair</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Review  Sentiment\n",
       "0           super!          1\n",
       "1          awesome          1\n",
       "2             fair          0\n",
       "3  useless product          0\n",
       "4             fair          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1653983761630,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "tmBZbWOCR-p3",
    "outputId": "8cb1ae56-ca7e-47fe-aa4a-149472ac5168"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "1    160659\n",
       "0     44390\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check distribution of sentiments\n",
    "dataset['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1653983782469,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "_Cuulg5nT9Fd",
    "outputId": "6d06a89a-8dfd-4a73-a2bb-27cc4af3a2aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review       24664\n",
       "Sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "dataset.isnull().sum()\n",
    "\n",
    "# no null values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rpate\\AppData\\Local\\Temp\\ipykernel_36840\\1935554401.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['Review'].fillna(' ', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dataset['Review'].fillna(' ', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uOoNB4vUhiv"
   },
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XCPpUC--BNLW"
   },
   "outputs": [],
   "source": [
    "x = dataset['Review']\n",
    "y = dataset['Sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTfcWsAuWCSN",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1653983847775,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "9bftGN8g11iC",
    "outputId": "c082d154-9ba8-4a02-8c2a-46aa13bdb4a8"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import string\n",
    "punct = string.punctuation\n",
    "# punct\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = list(STOP_WORDS) # list of stopwords\n",
    "\n",
    "class CustomTokenizerExample():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def text_data_cleaning(self,sentence):\n",
    "        doc = nlp(sentence)                         # spaCy tokenize text & call doc components, in order\n",
    "\n",
    "        tokens = [] # list of tokens\n",
    "        for token in doc:\n",
    "            if token.lemma_ != \"-PRON-\":\n",
    "                temp = token.lemma_.lower().strip()\n",
    "            else:\n",
    "              temp = token.lower_\n",
    "            tokens.append(temp)\n",
    "\n",
    "        cleaned_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in stopwords and token not in punct:\n",
    "                cleaned_tokens.append(token)\n",
    "        return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first',\n",
       " 'bottom',\n",
       " 'must',\n",
       " 'therefore',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'off',\n",
       " 'them',\n",
       " 'ours',\n",
       " 'do',\n",
       " 'per',\n",
       " 'in',\n",
       " 'almost',\n",
       " 'themselves',\n",
       " 'for',\n",
       " 'that',\n",
       " 'but',\n",
       " 'twelve',\n",
       " 'somehow',\n",
       " 'eleven',\n",
       " 'seeming',\n",
       " 'whereby',\n",
       " 'â€™re',\n",
       " 'nothing',\n",
       " 'every',\n",
       " 'just',\n",
       " 'last',\n",
       " 'they',\n",
       " 'hereby',\n",
       " 'several',\n",
       " 'their',\n",
       " 'elsewhere',\n",
       " 'too',\n",
       " 'whence',\n",
       " 'against',\n",
       " 'no',\n",
       " 'amongst',\n",
       " 'again',\n",
       " 'even',\n",
       " 'each',\n",
       " 'though',\n",
       " 'anyway',\n",
       " 'part',\n",
       " 'unless',\n",
       " 'some',\n",
       " 'due',\n",
       " 'himself',\n",
       " 'mostly',\n",
       " 'where',\n",
       " 'me',\n",
       " 'his',\n",
       " 'ourselves',\n",
       " 'all',\n",
       " 'give',\n",
       " 'see',\n",
       " 'done',\n",
       " 'between',\n",
       " 'everything',\n",
       " 'indeed',\n",
       " 'few',\n",
       " 'wherever',\n",
       " 'many',\n",
       " 'am',\n",
       " 'my',\n",
       " 'although',\n",
       " 'enough',\n",
       " 'further',\n",
       " 'nor',\n",
       " 'anyone',\n",
       " 'various',\n",
       " \"'s\",\n",
       " 'became',\n",
       " 'such',\n",
       " 'herself',\n",
       " 'whoever',\n",
       " 'take',\n",
       " 'your',\n",
       " 'side',\n",
       " 'becoming',\n",
       " 'down',\n",
       " 'toward',\n",
       " 'had',\n",
       " 'was',\n",
       " 'ca',\n",
       " 'than',\n",
       " 'whatever',\n",
       " 'beside',\n",
       " 'the',\n",
       " 'we',\n",
       " 'she',\n",
       " 'throughout',\n",
       " 'have',\n",
       " 'very',\n",
       " 'been',\n",
       " 'were',\n",
       " 'whereas',\n",
       " 'made',\n",
       " 'go',\n",
       " 'often',\n",
       " 'become',\n",
       " 'there',\n",
       " 'is',\n",
       " 'regarding',\n",
       " 'itself',\n",
       " 'perhaps',\n",
       " 'nowhere',\n",
       " 'hence',\n",
       " 'already',\n",
       " 'whereafter',\n",
       " 'around',\n",
       " 'does',\n",
       " 'put',\n",
       " 'latterly',\n",
       " 'it',\n",
       " 'sometimes',\n",
       " 'own',\n",
       " 'will',\n",
       " 'nâ€™t',\n",
       " 'show',\n",
       " 'hers',\n",
       " \"'re\",\n",
       " 'everywhere',\n",
       " 'while',\n",
       " 'formerly',\n",
       " 'thereafter',\n",
       " 'keep',\n",
       " 'of',\n",
       " 'up',\n",
       " 'amount',\n",
       " 'much',\n",
       " 'i',\n",
       " 'move',\n",
       " 'doing',\n",
       " 'thereby',\n",
       " 'whereupon',\n",
       " 'always',\n",
       " 'more',\n",
       " 'beyond',\n",
       " 'still',\n",
       " 'after',\n",
       " 'who',\n",
       " 'herein',\n",
       " 'twenty',\n",
       " 'did',\n",
       " 'fifty',\n",
       " 'within',\n",
       " 'nevertheless',\n",
       " 'also',\n",
       " 'across',\n",
       " 'thence',\n",
       " 'â€˜re',\n",
       " 'mine',\n",
       " 'any',\n",
       " 'nine',\n",
       " 'yours',\n",
       " 'should',\n",
       " 'â€˜s',\n",
       " 'â€™m',\n",
       " 'please',\n",
       " 'moreover',\n",
       " 'empty',\n",
       " 'least',\n",
       " 'well',\n",
       " 'â€™ve',\n",
       " 'therein',\n",
       " 'using',\n",
       " 'to',\n",
       " 'until',\n",
       " 'quite',\n",
       " 'â€™ll',\n",
       " 'onto',\n",
       " 'same',\n",
       " 'seems',\n",
       " 'when',\n",
       " 'might',\n",
       " 'sometime',\n",
       " 'serious',\n",
       " 'somewhere',\n",
       " 'thru',\n",
       " 'towards',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'from',\n",
       " 'thereupon',\n",
       " 'anyhow',\n",
       " 'only',\n",
       " 'really',\n",
       " 'most',\n",
       " 'above',\n",
       " 'anything',\n",
       " 'top',\n",
       " 'an',\n",
       " 'four',\n",
       " 'either',\n",
       " 'during',\n",
       " 'among',\n",
       " 'a',\n",
       " 'five',\n",
       " 'seem',\n",
       " 'whom',\n",
       " 'behind',\n",
       " 'those',\n",
       " 'our',\n",
       " 'upon',\n",
       " 'under',\n",
       " 'through',\n",
       " 'back',\n",
       " \"'m\",\n",
       " 'name',\n",
       " 'another',\n",
       " \"n't\",\n",
       " 'and',\n",
       " 'may',\n",
       " 'into',\n",
       " 'â€™s',\n",
       " 'â€™d',\n",
       " 'hundred',\n",
       " 'because',\n",
       " 'him',\n",
       " 'if',\n",
       " 'noone',\n",
       " 'latter',\n",
       " 'everyone',\n",
       " 'afterwards',\n",
       " 'then',\n",
       " 'front',\n",
       " 'else',\n",
       " 'make',\n",
       " 'yet',\n",
       " 'namely',\n",
       " 'third',\n",
       " 'whether',\n",
       " 'her',\n",
       " 'here',\n",
       " 'us',\n",
       " 'sixty',\n",
       " 'thus',\n",
       " 'once',\n",
       " 'whole',\n",
       " 'â€˜d',\n",
       " 'along',\n",
       " 'other',\n",
       " 'whose',\n",
       " 'next',\n",
       " 'otherwise',\n",
       " 'what',\n",
       " 'someone',\n",
       " 'besides',\n",
       " 'two',\n",
       " \"'d\",\n",
       " 'ten',\n",
       " 'full',\n",
       " 'hereupon',\n",
       " 'hereafter',\n",
       " 'about',\n",
       " 'something',\n",
       " 'you',\n",
       " 'others',\n",
       " 'so',\n",
       " 'by',\n",
       " 'less',\n",
       " 'becomes',\n",
       " 'why',\n",
       " 'with',\n",
       " 'six',\n",
       " 'say',\n",
       " 'forty',\n",
       " 'never',\n",
       " 'one',\n",
       " 'below',\n",
       " 'rather',\n",
       " 'nâ€˜t',\n",
       " 'on',\n",
       " 'cannot',\n",
       " 'both',\n",
       " 'whither',\n",
       " 'has',\n",
       " 'three',\n",
       " 'call',\n",
       " 'its',\n",
       " 'out',\n",
       " 'can',\n",
       " 'neither',\n",
       " 'without',\n",
       " 'which',\n",
       " 'before',\n",
       " 'being',\n",
       " 'as',\n",
       " 'or',\n",
       " 'eight',\n",
       " \"'ll\",\n",
       " 'used',\n",
       " 'meanwhile',\n",
       " 'be',\n",
       " 'are',\n",
       " 'however',\n",
       " 'nobody',\n",
       " 'at',\n",
       " 'beforehand',\n",
       " 'these',\n",
       " 'seemed',\n",
       " 'not',\n",
       " 'yourself',\n",
       " 're',\n",
       " 'myself',\n",
       " 'over',\n",
       " 'would',\n",
       " 'could',\n",
       " 'alone',\n",
       " 'whenever',\n",
       " 'wherein',\n",
       " 'get',\n",
       " 'â€˜m',\n",
       " 'â€˜ll',\n",
       " 'fifteen',\n",
       " 'none',\n",
       " 'â€˜ve',\n",
       " 'how',\n",
       " \"'ve\",\n",
       " 'via',\n",
       " 'anywhere',\n",
       " 'this',\n",
       " 'except',\n",
       " 'together',\n",
       " 'now',\n",
       " 'former']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uKsnnULWzDAn"
   },
   "outputs": [],
   "source": [
    "# if root form of that word is not proper noun then it is going to convert that into lower form\n",
    "# and if that word is a proper noun, then we are directly taking lower form,\n",
    "# because there is no lemma for proper noun\n",
    "\n",
    "# stopwords and punctuations removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1653984108803,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "VHgVcoYFyStv",
    "outputId": "41b69e91-fa49-482e-b084-b84a00136b18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'day', 'life']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's do a test\n",
    "token = CustomTokenizerExample()\n",
    "token.text_data_cleaning(\"Those were the best days of my life!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb-4E3Xj0MZI"
   },
   "source": [
    "### Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EHlWW9D10XwB"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3OZuzO9c0mhq"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "# tokenizer=text_data_cleaning, tokenization will be done according to this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaJPf_MWAiUk"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BkEiNIFwBfgM"
   },
   "outputs": [],
   "source": [
    "# Train_Test_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Qeqrb1Es1LyF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.8, stratify = dataset.Sentiment, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1653984441648,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "DUX2gNXl1Q8c",
    "outputId": "1aba0197-2aa2-475a-af44-2d9285943ef8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41009,), (164040,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205049,) (41009,) (164040,) (41009,) (164040,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, x_train.shape, x_test.shape,y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127870             excellent\n",
       "204911    highly recommended\n",
       "66452      terrific purchase\n",
       "171850             very good\n",
       "59308                       \n",
       "                 ...        \n",
       "626                wonderful\n",
       "86183              hated it!\n",
       "137832                  nice\n",
       "121919    highly recommended\n",
       "101630        classy product\n",
       "Name: Review, Length: 41009, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['e'],\n",
       " ['x'],\n",
       " ['c'],\n",
       " ['e'],\n",
       " ['l'],\n",
       " ['l'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['t'],\n",
       " ['h'],\n",
       " [],\n",
       " ['g'],\n",
       " ['h'],\n",
       " ['l'],\n",
       " ['y'],\n",
       " [],\n",
       " ['r'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['d'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['r'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['c'],\n",
       " [],\n",
       " ['p'],\n",
       " ['u'],\n",
       " ['r'],\n",
       " ['c'],\n",
       " ['h'],\n",
       " [],\n",
       " ['s'],\n",
       " ['e'],\n",
       " ['v'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['y'],\n",
       " [],\n",
       " ['g'],\n",
       " ['o'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " [],\n",
       " ['s'],\n",
       " ['u'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " [],\n",
       " [],\n",
       " ['w'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " [],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['e'],\n",
       " ['d'],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['r'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['d'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['s'],\n",
       " [],\n",
       " ['m'],\n",
       " ['p'],\n",
       " ['l'],\n",
       " ['y'],\n",
       " [],\n",
       " [],\n",
       " ['w'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['e'],\n",
       " ['d'],\n",
       " [],\n",
       " ['g'],\n",
       " ['o'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " [],\n",
       " ['c'],\n",
       " ['h'],\n",
       " ['o'],\n",
       " [],\n",
       " ['c'],\n",
       " ['e'],\n",
       " ['v'],\n",
       " [],\n",
       " ['l'],\n",
       " ['u'],\n",
       " ['e'],\n",
       " [],\n",
       " ['f'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " [],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['e'],\n",
       " ['y'],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['g'],\n",
       " ['o'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['e'],\n",
       " ['d'],\n",
       " ['w'],\n",
       " [],\n",
       " ['s'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " [],\n",
       " ['o'],\n",
       " ['f'],\n",
       " [],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['e'],\n",
       " ['y'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['w'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " ['l'],\n",
       " [],\n",
       " ['s'],\n",
       " ['s'],\n",
       " ['y'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['g'],\n",
       " ['r'],\n",
       " ['e'],\n",
       " [],\n",
       " ['t'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['b'],\n",
       " ['r'],\n",
       " [],\n",
       " ['l'],\n",
       " ['l'],\n",
       " [],\n",
       " [],\n",
       " ['n'],\n",
       " ['t'],\n",
       " [],\n",
       " ['w'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " ['j'],\n",
       " ['u'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['w'],\n",
       " [],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['r'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['d'],\n",
       " [],\n",
       " [],\n",
       " ['t'],\n",
       " [],\n",
       " [],\n",
       " ['l'],\n",
       " ['l'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['t'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['v'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['y'],\n",
       " [],\n",
       " ['g'],\n",
       " ['o'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['c'],\n",
       " ['o'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['d'],\n",
       " [],\n",
       " ['b'],\n",
       " ['e'],\n",
       " [],\n",
       " ['w'],\n",
       " [],\n",
       " ['y'],\n",
       " [],\n",
       " ['b'],\n",
       " ['e'],\n",
       " ['t'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " [],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " ['e'],\n",
       " ['x'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " [],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['c'],\n",
       " ['e'],\n",
       " [],\n",
       " ['e'],\n",
       " ['v'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " [],\n",
       " ['j'],\n",
       " ['u'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['w'],\n",
       " [],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['e'],\n",
       " ['d'],\n",
       " ['j'],\n",
       " ['u'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " ['o'],\n",
       " ['k'],\n",
       " [],\n",
       " ['y'],\n",
       " ['s'],\n",
       " [],\n",
       " ['m'],\n",
       " ['p'],\n",
       " ['l'],\n",
       " ['y'],\n",
       " [],\n",
       " [],\n",
       " ['w'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['r'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['c'],\n",
       " ['f'],\n",
       " [],\n",
       " ['b'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['o'],\n",
       " ['u'],\n",
       " ['s'],\n",
       " [],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['l'],\n",
       " [],\n",
       " ['g'],\n",
       " ['h'],\n",
       " ['t'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " [],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['e'],\n",
       " ['d'],\n",
       " [],\n",
       " ['b'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['l'],\n",
       " ['u'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " [],\n",
       " ['r'],\n",
       " ['u'],\n",
       " ['b'],\n",
       " ['b'],\n",
       " [],\n",
       " ['s'],\n",
       " ['h'],\n",
       " [],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['e'],\n",
       " ['d'],\n",
       " ['g'],\n",
       " ['r'],\n",
       " ['e'],\n",
       " [],\n",
       " ['t'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " [],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['r'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['c'],\n",
       " [],\n",
       " ['p'],\n",
       " ['u'],\n",
       " ['r'],\n",
       " ['c'],\n",
       " ['h'],\n",
       " [],\n",
       " ['s'],\n",
       " ['e'],\n",
       " [],\n",
       " ['n'],\n",
       " [],\n",
       " ['c'],\n",
       " ['e'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['u'],\n",
       " ['n'],\n",
       " ['s'],\n",
       " [],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['f'],\n",
       " [],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " ['y'],\n",
       " ['s'],\n",
       " [],\n",
       " ['m'],\n",
       " ['p'],\n",
       " ['l'],\n",
       " ['y'],\n",
       " [],\n",
       " [],\n",
       " ['w'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " [],\n",
       " ['v'],\n",
       " [],\n",
       " ['l'],\n",
       " ['u'],\n",
       " ['e'],\n",
       " [],\n",
       " ['f'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " [],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['e'],\n",
       " ['y'],\n",
       " ['b'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " [],\n",
       " ['n'],\n",
       " [],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [],\n",
       " ['m'],\n",
       " [],\n",
       " ['r'],\n",
       " ['k'],\n",
       " ['e'],\n",
       " ['t'],\n",
       " [],\n",
       " ['c'],\n",
       " ['l'],\n",
       " [],\n",
       " ['s'],\n",
       " ['s'],\n",
       " ['y'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['e'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['n'],\n",
       " ['s'],\n",
       " [],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['f'],\n",
       " [],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " ['y'],\n",
       " ['d'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " [],\n",
       " ['t'],\n",
       " [],\n",
       " ['w'],\n",
       " [],\n",
       " ['s'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " [],\n",
       " ['y'],\n",
       " ['o'],\n",
       " ['u'],\n",
       " ['r'],\n",
       " [],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['e'],\n",
       " ['y'],\n",
       " ['j'],\n",
       " ['u'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['w'],\n",
       " [],\n",
       " ['d'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " [],\n",
       " ['t'],\n",
       " [],\n",
       " ['w'],\n",
       " [],\n",
       " ['s'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " [],\n",
       " ['y'],\n",
       " ['o'],\n",
       " ['u'],\n",
       " ['r'],\n",
       " [],\n",
       " ['m'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['e'],\n",
       " ['y'],\n",
       " ['h'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " ['r'],\n",
       " [],\n",
       " ['b'],\n",
       " ['l'],\n",
       " ['e'],\n",
       " [],\n",
       " [],\n",
       " ['w'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " [],\n",
       " ['c'],\n",
       " ['e'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['x'],\n",
       " ['c'],\n",
       " ['e'],\n",
       " ['l'],\n",
       " ['l'],\n",
       " ['e'],\n",
       " ['n'],\n",
       " ['t'],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['e'],\n",
       " ['d'],\n",
       " ['s'],\n",
       " [],\n",
       " ['m'],\n",
       " ['p'],\n",
       " ['l'],\n",
       " ['y'],\n",
       " [],\n",
       " [],\n",
       " ['w'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['o'],\n",
       " ['m'],\n",
       " ['e'],\n",
       " [],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['l'],\n",
       " [],\n",
       " ['g'],\n",
       " ['h'],\n",
       " ['t'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['s'],\n",
       " ['u'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['r'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['c'],\n",
       " [],\n",
       " ['p'],\n",
       " ['u'],\n",
       " ['r'],\n",
       " ['c'],\n",
       " ['h'],\n",
       " [],\n",
       " ['s'],\n",
       " ['e'],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['b'],\n",
       " ['r'],\n",
       " [],\n",
       " ['l'],\n",
       " ['l'],\n",
       " [],\n",
       " [],\n",
       " ['n'],\n",
       " ['t'],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['r'],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['l'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['s'],\n",
       " ['n'],\n",
       " ['o'],\n",
       " ['t'],\n",
       " [],\n",
       " ['s'],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['e'],\n",
       " ['d'],\n",
       " ['j'],\n",
       " ['u'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['w'],\n",
       " [],\n",
       " ['b'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " [],\n",
       " ['n'],\n",
       " [],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [],\n",
       " ['m'],\n",
       " [],\n",
       " ['r'],\n",
       " ['k'],\n",
       " ['e'],\n",
       " ['t'],\n",
       " [],\n",
       " ['p'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['e'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " [],\n",
       " ['t'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['r'],\n",
       " [],\n",
       " ['f'],\n",
       " [],\n",
       " ['c'],\n",
       " [],\n",
       " ['b'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " [],\n",
       " ['n'],\n",
       " [],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [],\n",
       " ['m'],\n",
       " [],\n",
       " ['r'],\n",
       " ['k'],\n",
       " ['e'],\n",
       " ['t'],\n",
       " [],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " [],\n",
       " ['g'],\n",
       " ['r'],\n",
       " ['e'],\n",
       " [],\n",
       " ['t'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['w'],\n",
       " ['o'],\n",
       " ['n'],\n",
       " ['d'],\n",
       " ['e'],\n",
       " ['r'],\n",
       " ['f'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['f'],\n",
       " [],\n",
       " ['b'],\n",
       " ['u'],\n",
       " ['l'],\n",
       " ['o'],\n",
       " ['u'],\n",
       " ['s'],\n",
       " [],\n",
       " ['g'],\n",
       " ['o'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " [],\n",
       " ['q'],\n",
       " ['u'],\n",
       " [],\n",
       " ['l'],\n",
       " [],\n",
       " ['t'],\n",
       " ['y'],\n",
       " [],\n",
       " ['p'],\n",
       " ['r'],\n",
       " ['o'],\n",
       " ['d'],\n",
       " ['u'],\n",
       " ['c'],\n",
       " ['t'],\n",
       " ['b'],\n",
       " ['e'],\n",
       " ['s'],\n",
       " ['t'],\n",
       " [],\n",
       " [],\n",
       " ['n'],\n",
       " [],\n",
       " ['t'],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [],\n",
       " ['m'],\n",
       " [],\n",
       " ['r'],\n",
       " ['k'],\n",
       " ['e'],\n",
       " ['t'],\n",
       " [],\n",
       " ['v'],\n",
       " [],\n",
       " ['l'],\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = []\n",
    "\n",
    "for i in x_train:\n",
    "    for j in i:\n",
    "        cleaned_text = token.text_data_cleaning(j)\n",
    "        text.append(cleaned_text)\n",
    "\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rpate\\Downloads\\p-1\\model\\SentimentAnalysis_project.ipynb Cell 42\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rpate/Downloads/p-1/model/SentimentAnalysis_project.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tfidf_matrix \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39;49mfit_transform(text)\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2119\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2121\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2122\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2123\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2124\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2125\u001b[0m )\n\u001b[1;32m-> 2126\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2128\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1375\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1376\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1380\u001b[0m             )\n\u001b[0;32m   1381\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1383\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1386\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1269\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1270\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1271\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:68\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 68\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     69\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidf.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'excellent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rpate\\Downloads\\p-1\\model\\SentimentAnalysis_project.ipynb Cell 44\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rpate/Downloads/p-1/model/SentimentAnalysis_project.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Create and fit a logistic regression model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rpate/Downloads/p-1/model/SentimentAnalysis_project.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m logistic_classifier \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rpate/Downloads/p-1/model/SentimentAnalysis_project.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m logistic_classifier\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rpate/Downloads/p-1/model/SentimentAnalysis_project.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rpate/Downloads/p-1/model/SentimentAnalysis_project.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x_test_tfidf \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mtransform(x_test)\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1205\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[1;32m-> 1207\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1208\u001b[0m     X,\n\u001b[0;32m   1209\u001b[0m     y,\n\u001b[0;32m   1210\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1211\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[0;32m   1212\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1213\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1214\u001b[0m )\n\u001b[0;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "File \u001b[1;32mc:\\Users\\rpate\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:917\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[39mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[39m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    916\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m--> 917\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(values, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m using_copy_on_write() \u001b[39mand\u001b[39;00m astype_is_view(values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m    919\u001b[0m     arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mview()\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'excellent'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression  # Import LogisticRegression\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=token.text_data_cleaning)\n",
    "x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "\n",
    "# Create and fit a logistic regression model\n",
    "logistic_classifier = LogisticRegression()\n",
    "logistic_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "x_test_tfidf = tfidf.transform(x_test)\n",
    "predictions = logistic_classifier.predict(x_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xp3Nj55h5smN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1653984612602,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "xvt59nSM58ee",
    "outputId": "e0f881b3-38bd-44d0-dad8-892b4a8c049c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5963,  2915],\n",
       "       [   31, 32101]], dtype=int64)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion_matrix\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1653984618247,
     "user": {
      "displayName": "SKILLCATE",
      "userId": "11062674699417926870"
     },
     "user_tz": -330
    },
    "id": "CoX7Jabd6Ar1",
    "outputId": "f35ad9d1-c76b-46e6-b59d-50d3033ad591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.67      0.80      8878\n",
      "           1       0.92      1.00      0.96     32132\n",
      "\n",
      "    accuracy                           0.93     41010\n",
      "   macro avg       0.96      0.84      0.88     41010\n",
      "weighted avg       0.93      0.93      0.92     41010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5cosLk16ChG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpo6jTiNaXYu"
   },
   "source": [
    "# Predict Sentiments using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "review = 'bad review'\n",
    "\n",
    "# Transform the review using the same TF-IDF vectorizer\n",
    "review_tfidf = tfidf.transform([review])\n",
    "\n",
    "# Make a prediction for the review\n",
    "prediction = logistic_classifier.predict(review_tfidf)\n",
    "\n",
    "# Print the prediction\n",
    "print(\"Predicted class:\", prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model and vectorizer to a pickle file\n",
    "\n",
    "with open('sentiment_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(logistic_classifier, model_file)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(tfidf, vectorizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model and vectorizer to a pickle file\n",
    "\n",
    "with open('sentiment_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(logistic_classifier, model_file)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(tfidf, vectorizer_file)\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as tokenizer_file:\n",
    "    pickle.dump(token, tokenizer_file)\n",
    "\n",
    "# Load the model and vectorizer from the pickle files\n",
    "with open('sentiment_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as vectorizer_file:\n",
    "    loaded_vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "with open('model/tokenizer.pkl', 'rb') as tokenizer_file:\n",
    "    oaded_tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "\n",
    "# Example review for prediction\n",
    "example_review = 'this is my bad review'\n",
    "\n",
    "# Transform the example review using the loaded vectorizer\n",
    "example_review_tfidf = loaded_vectorizer.transform([example_review])\n",
    "\n",
    "# Make predictions\n",
    "prediction = loaded_model.predict(example_review_tfidf)\n",
    "\n",
    "# Display the prediction\n",
    "if prediction[0] == 1:\n",
    "    print('Positive sentiment')\n",
    "else:\n",
    "    print('Negative sentiment')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1Dsyd5zB23upbCRDxlrh_r1P1wIlILCEa",
     "timestamp": 1642754413575
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
